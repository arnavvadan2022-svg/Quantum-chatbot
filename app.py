from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
import os

from src.arxiv_search import ArxivSearcher
from src.serpapi_search import SerpAPISearcher
from src.google_search import GoogleSearcher
from src.web_scraper import WebScraper
from src.query_processor import QueryProcessor
from src.rag_engine import RAGEngine

# Load environment variables
load_dotenv()

app = Flask(__name__)
app.config['SECRET_KEY'] = os.getenv('FLASK_SECRET_KEY', 'dev-secret-key')
CORS(app)

# Initialize components
arxiv_searcher = ArxivSearcher()
serpapi_searcher = SerpAPISearcher(os.getenv('SERPAPI_KEY'))
google_searcher = GoogleSearcher(
    api_key=os.getenv('GOOGLE_API_KEY'),
    cse_id=os.getenv('GOOGLE_CSE_ID')
)
web_scraper = WebScraper()
query_processor = QueryProcessor()

# Initialize RAG engine
print("\n[INIT] Initializing RAG Engine...")
rag_engine = RAGEngine(groq_api_key=os.getenv('GROQ_API_KEY'))


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/api/query', methods=['POST'])
def process_query():
    try:
        data = request.json
        user_query = data.get('query', '')

        if not user_query:
            return jsonify({'error': 'No query provided'}), 400

        # Process and validate query
        processed_query = query_processor.process(user_query)

        if not query_processor.is_quantum_related(processed_query):
            return jsonify({
                'error': 'Query must be related to quantum mechanics or quantum computing'
            }), 400

        print(f"\n{'=' * 70}")
        print(f"[RAG PIPELINE] Query: {processed_query}")
        print(f"{'=' * 70}\n")

        # Step 1: Retrieve from multiple sources
        print("üìö Step 1: Multi-source retrieval...")
        arxiv_results = arxiv_searcher.search(processed_query)
        print(f"   ‚úì arXiv: {len(arxiv_results)} papers")

        all_web_results = []

        if serpapi_searcher.is_configured():
            serpapi_results = serpapi_searcher.search(processed_query)
            all_web_results.extend(serpapi_results)
            print(f"   ‚úì SerpAPI: {len(serpapi_results)} results")

        if google_searcher.is_configured():
            google_results = google_searcher.search(processed_query)
            all_web_results.extend(google_results)
            print(f"   ‚úì Google: {len(google_results)} results")

        web_results = web_scraper.search_all(processed_query)
        all_web_results.extend(web_results)
        print(f"   ‚úì Web Scraping: {len(web_results)} results")

        # Step 2: Prepare documents for RAG
        all_documents = []

        for paper in arxiv_results:
            all_documents.append({
                'title': paper['title'],
                'snippet': paper['summary'][:700],
                'link': paper['link'],
                'source_type': 'arXiv'
            })

        for result in all_web_results:
            all_documents.append({
                'title': result['title'],
                'snippet': result['snippet'][:700],
                'link': result['link'],
                'source_type': result.get('source', 'Web')
            })

        print(f"\nüîç Step 2: Indexing {len(all_documents)} documents...")

        # Step 3: Add to vector DB
        rag_engine.reset()
        rag_engine.add_documents(all_documents)

        # Step 4: Semantic search
        print(f"\nüéØ Step 3: Semantic search...")
        retrieved_docs = rag_engine.retrieve(processed_query, top_k=8)

        # Step 5: Generate answer with LLM
        print(f"\nü§ñ Step 4: Generating answer...")
        result = rag_engine.generate_answer(processed_query, retrieved_docs)

        print(f"\n‚úÖ Complete! Generated by: {result['generated_by']}")
        print(f"{'=' * 70}\n")

        return jsonify({
            'success': True,
            'query': user_query,
            'structured_answer': result['structured_answer'],
            'sources': result['sources'],
            'confidence': result['confidence'],
            'debug': {
                'arxiv_count': len(arxiv_results),
                'web_count': len(all_web_results),
                'total_docs': len(all_documents),
                'retrieved_docs': len(retrieved_docs),
                'generated_by': result['generated_by']
            }
        })

    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'components': {
            'arxiv': True,
            'serpapi': serpapi_searcher.is_configured(),
            'google_cse': google_searcher.is_configured(),
            'web_scraping': True,
            'rag_engine': True,
            'llm': rag_engine.llm_available,
            'llm_model': rag_engine.model_name if rag_engine.llm_available else None
        }
    })


if __name__ == '__main__':
    print("\n" + "=" * 70)
    print("üöÄ QUANTUM CHATBOT WITH FREE RAG")
    print("=" * 70)
    print(f"üìö Data Sources:")
    print(f"   ‚úì arXiv Research Papers")
    print(f"   {'‚úì' if serpapi_searcher.is_configured() else '‚óã'} SerpAPI")
    print(f"   {'‚úì' if google_searcher.is_configured() else '‚óã'} Google Custom Search")
    print(f"   ‚úì Web Scraping (Wikipedia, IBM, Qiskit)")
    print(f"\nü§ñ RAG Components:")
    print(f"   ‚úì Embeddings: sentence-transformers (local)")
    print(f"   ‚úì Vector DB: ChromaDB (in-memory)")
    print(
        f"   {'‚úì' if rag_engine.llm_available else '‚óã'} LLM: {rag_engine.model_name if rag_engine.llm_available else 'Not configured'}")
    print("=" * 70 + "\n")

    if not rag_engine.llm_available:
        print("üí° TIP: Get free Groq API key for AI-generated answers:")
        print("   1. Sign up: https://console.groq.com/keys")
        print("   2. Copy API key to .env as GROQ_API_KEY")
        print("   3. Restart app\n")

    port = int(os.environ.get('PORT', 5000))
    app.run(debug=False, host='0.0.0.0', port=port)